{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd8c06e9",
   "metadata": {},
   "source": [
    "# Old Attempted Strat - Embedding Category Names into CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3526fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "category_dict = {\n",
    "\t'Boots': ['Ankle','Knee High','Mid-Calf','Over the Knee','Prewalker Boots'],\n",
    " \t'Sandals': ['Athletic', 'Flat', 'Heel'],\n",
    "\t'Shoes': ['Boat Shoes','Clogs and Mules','Crib Shoes','Firstwalker','Flats','Heels','Loafers','Oxfords','Prewalker','Sneakers and Athletic Shoes'],\n",
    "\t'Slippers': ['Boot', 'Slipper Flats', 'Slipper Heels']\n",
    "}\n",
    "\n",
    "# 1. Load CLIP\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "def encode_several_texts(lst):\n",
    "\ttext_tokens = clip.tokenize(lst).to(device)\n",
    "\t# Encode the texts\n",
    "\twith torch.no_grad():\n",
    "\t\ttext_features = model.encode_text(text_tokens)\n",
    "\t# Normalize\n",
    "\treturn text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "encoded_categories = encode_several_texts(category_dict.keys())\n",
    "encoded_sub_categories = {}\n",
    "for category in category_dict:\n",
    "\tencoded_sub_categories[category] = encode_several_texts(category_dict[category])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe8f48b",
   "metadata": {},
   "source": [
    "# Encoding images with CLIP and running similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5531214f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sandals Flat 0.27488837\n"
     ]
    }
   ],
   "source": [
    "def encode_image_given_image_path(image_path):\n",
    "\timage = preprocess(Image.open(image_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "\n",
    "\t# 3. Encode the image\n",
    "\twith torch.no_grad():\n",
    "\t\timage_features = model.encode_image(image)\n",
    "\t# normalize to unit length\n",
    "\treturn image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "image_path = \"test2.jpg\"\n",
    "image_features = encode_image_given_image_path(image_path)\n",
    "\n",
    "\n",
    "category_similarities = (image_features @ encoded_categories.T).squeeze(0).cpu().numpy()\n",
    "category = list(category_dict.keys())[category_similarities.argmax()]\n",
    "\n",
    "subcategory_similarities = (image_features @ encoded_sub_categories[category].T).squeeze(0).cpu().numpy()\n",
    "subcategory = category_dict[category][category_similarities.argmax()]\n",
    "print(category, subcategory, category_similarities.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034006d7",
   "metadata": {},
   "source": [
    "# Forward Encoding all images in the dataset\n",
    "## (Prerequesite to begin training our MLP layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6025d2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50066 JPGs under shoes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding images: 100%|██████████| 50066/50066 [1:32:44<00:00,  9.00img/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. CPU‐only setup\n",
    "device = \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "model.eval()\n",
    "\n",
    "# 2. Gather all JPG paths\n",
    "root = Path(\"shoes\")\n",
    "all_jpgs = list(root.rglob(\"*.jpg\"))\n",
    "print(f\"Found {len(all_jpgs)} JPGs under {root}\")\n",
    "\n",
    "# 3. Prepare output folder\n",
    "out_dir = Path(\"shoe_features_cpu\")\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# 4. Process one by one\n",
    "with torch.no_grad():\n",
    "\tfor img_path in tqdm(all_jpgs, desc=\"Encoding images\", unit=\"img\"):\n",
    "\t\t# skip if already done\n",
    "\t\tout_file = out_dir / f\"{img_path.stem}.npy\"\n",
    "\t\tif out_file.exists():\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\t# load + preprocess\n",
    "\t\timg = Image.open(img_path).convert(\"RGB\")\n",
    "\t\timg_t = preprocess(img).unsqueeze(0).to(device)  # 1×3×224×224\n",
    "\n",
    "\t\t# forward pass\n",
    "\t\tfeat = model.encode_image(img_t)                 # 1×512\n",
    "\t\tfeat = feat / feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "\t\t# save vector\n",
    "\t\tnp.save(out_file, feat.cpu().numpy().squeeze())\n",
    "\n",
    "print(\"All done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb99511",
   "metadata": {},
   "source": [
    "# Training our MLP weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274f4885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning feature files …\n",
      "Discovered 21 classes over 50031 samples. Loading …\n",
      "Train: 45028  |  Val: 5003\n",
      "Epoch  1 DONE | TrainLoss 1.9367 | ValAcc 64.86%\n",
      "  ↳ saved new best model\n",
      "Epoch  2 DONE | TrainLoss 0.9878 | ValAcc 73.90%\n",
      "  ↳ saved new best model\n",
      "Epoch  3 DONE | TrainLoss 0.7858 | ValAcc 76.41%\n",
      "  ↳ saved new best model\n",
      "Epoch  4 DONE | TrainLoss 0.7086 | ValAcc 77.41%\n",
      "  ↳ saved new best model\n",
      "Epoch  5 DONE | TrainLoss 0.6638 | ValAcc 78.51%\n",
      "  ↳ saved new best model\n",
      "Epoch  6 DONE | TrainLoss 0.6335 | ValAcc 79.09%\n",
      "  ↳ saved new best model\n",
      "Epoch  7 DONE | TrainLoss 0.6109 | ValAcc 79.43%\n",
      "  ↳ saved new best model\n",
      "Epoch  8 DONE | TrainLoss 0.5936 | ValAcc 79.95%\n",
      "  ↳ saved new best model\n",
      "Epoch  9 DONE | TrainLoss 0.5795 | ValAcc 80.45%\n",
      "  ↳ saved new best model\n",
      "Epoch 10 DONE | TrainLoss 0.5684 | ValAcc 80.35%\n",
      "Epoch 11 DONE | TrainLoss 0.5585 | ValAcc 80.77%\n",
      "  ↳ saved new best model\n",
      "Epoch 12 DONE | TrainLoss 0.5504 | ValAcc 80.91%\n",
      "  ↳ saved new best model\n",
      "Epoch 13 DONE | TrainLoss 0.5420 | ValAcc 81.07%\n",
      "  ↳ saved new best model\n",
      "Epoch 14 DONE | TrainLoss 0.5359 | ValAcc 81.43%\n",
      "  ↳ saved new best model\n",
      "Epoch 15 DONE | TrainLoss 0.5291 | ValAcc 81.43%\n",
      "\n",
      "Best validation accuracy: 81.43%\n",
      "Class mapping saved above.  Script finished.\n"
     ]
    }
   ],
   "source": [
    "# ---------- Config ----------\n",
    "FEAT_ROOT  = Path(\"shoe_features\")\n",
    "HIDDEN_DIM = 256\n",
    "BATCH_SZ   = 256\n",
    "EPOCHS     = 15\n",
    "LR         = 5e-4\n",
    "VAL_SPLIT  = 0.10\n",
    "SEED       = 42\n",
    "DEVICE     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# -----------------------------\n",
    "\n",
    "def set_seed(s):\n",
    "    random.seed(s); np.random.seed(s); torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
    "set_seed(SEED)\n",
    "\n",
    "# ----- Step 1: load everything into RAM -----\n",
    "print(\"Scanning feature files …\")\n",
    "samples, class_to_idx, classes = [], {}, []\n",
    "for dirpath, dirnames, filenames in os.walk(FEAT_ROOT):\n",
    "    if filenames and not dirnames:                      # leaf dir\n",
    "        rel = Path(dirpath).relative_to(FEAT_ROOT)\n",
    "        cname = str(rel)                                # e.g. Boots/ankle_boots\n",
    "        if cname not in class_to_idx:\n",
    "            class_to_idx[cname] = len(classes)\n",
    "            classes.append(cname)\n",
    "\n",
    "        lbl = class_to_idx[cname]\n",
    "        for f in filenames:\n",
    "            if f.endswith(\".npy\"):\n",
    "                samples.append((Path(dirpath)/f, lbl))\n",
    "\n",
    "print(f\"Discovered {len(classes)} classes over {len(samples)} samples. Loading …\")\n",
    "\n",
    "features = np.zeros((len(samples), 512), dtype=np.float32)\n",
    "labels   = np.zeros(len(samples),       dtype=np.int64)\n",
    "\n",
    "for i, (path, lbl) in enumerate(samples):\n",
    "    features[i] = np.load(path)         # (512,)\n",
    "    labels[i]   = lbl\n",
    "\n",
    "# convert to tensors once\n",
    "features = torch.from_numpy(features)\n",
    "labels   = torch.from_numpy(labels)\n",
    "dataset  = TensorDataset(features, labels)\n",
    "\n",
    "# ----- Step 2: train/val split -----\n",
    "n_total = len(dataset)\n",
    "n_val   = int(VAL_SPLIT * n_total)\n",
    "n_train = n_total - n_val\n",
    "train_ds, val_ds = random_split(dataset, [n_train, n_val],\n",
    "                                generator=torch.Generator().manual_seed(SEED))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SZ, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SZ*2)\n",
    "\n",
    "print(f\"Train: {n_train}  |  Val: {n_val}\")\n",
    "\n",
    "# ----- Step 3: two-layer MLP probe -----\n",
    "class MLPProbe(nn.Module):\n",
    "    def __init__(self, d_in=512, d_hid=HIDDEN_DIM, n_cls=len(classes)):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_in, d_hid),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(d_hid, n_cls)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "model = MLPProbe().to(DEVICE)\n",
    "opt    = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "crit   = nn.CrossEntropyLoss()\n",
    "\n",
    "# ----- Step 4: training with a 10-second heartbeat -----\n",
    "best_acc = 0.0\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    processed    = 0\n",
    "    t_start      = time.time()\n",
    "    t_beat       = t_start\n",
    "\n",
    "    for feats, lbls in train_loader:\n",
    "        feats, lbls = feats.to(DEVICE), lbls.to(DEVICE)\n",
    "        logits = model(feats)\n",
    "        loss   = crit(logits, lbls)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        running_loss += loss.item() * feats.size(0)\n",
    "        processed    += feats.size(0)\n",
    "\n",
    "        # heartbeat every 10 s\n",
    "        now = time.time()\n",
    "        if now - t_beat >= 10:\n",
    "            pct = processed / n_train * 100\n",
    "            avg = running_loss / processed\n",
    "            print(f\"Epoch {epoch:2d}  {pct:6.2f}% | AvgLoss {avg:.4f}\", flush=True)\n",
    "            t_beat = now\n",
    "\n",
    "    # ---------- validation ----------\n",
    "    model.eval()\n",
    "    preds, gts = [], []\n",
    "    with torch.no_grad():\n",
    "        for feats, lbls in val_loader:\n",
    "            logits = model(feats.to(DEVICE))\n",
    "            preds.extend(logits.argmax(dim=-1).cpu().numpy())\n",
    "            gts.extend(lbls.numpy())\n",
    "\n",
    "    acc = accuracy_score(gts, preds)\n",
    "    train_loss = running_loss / n_train\n",
    "    print(f\"Epoch {epoch:2d} DONE | TrainLoss {train_loss:.4f} | ValAcc {acc*100:5.2f}%\")\n",
    "\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        torch.save(model.state_dict(), \"best_mlp_probe.pth\")\n",
    "        print(\"  ↳ saved new best model\")\n",
    "\n",
    "print(f\"\\nBest validation accuracy: {best_acc*100:.2f}%\")\n",
    "print(\"Class mapping saved above.  Script finished.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
